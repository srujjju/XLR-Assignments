# -*- coding: utf-8 -*-
"""RandomForest_Company

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DciSPfwSfSPkoLW0iR7Ka4o_T3cfSbvZ
"""

from google.colab import files

uploaded = files.upload()

import pandas as pd
file = pd.read_csv('Company_Data (1).csv')
file.head()

X = file.iloc[:,1:]
Y = file.iloc[:,0]

import seaborn as sns
import matplotlib.pyplot as plt

sns.pairplot(file)
plt.show()

# Calculate correlation matrix
correlation_matrix = file.corr()

# Visualize correlation matrix using heatmap
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm")
plt.show()

from sklearn.preprocessing import StandardScaler,LabelEncoder
SS = StandardScaler()
LR = LabelEncoder()
selected_columns_ss = ['CompPrice','Income','Advertising','Population','Price','Age','Education']
selected_columns_lr = ['ShelveLoc','Urban','US']
X[selected_columns_ss] = SS.fit_transform(X[selected_columns_ss])
for columns in selected_columns_lr:
  X[columns] = LR.fit_transform(X[columns])
X.head()

import numpy as np

sns.boxplot(data=X)
plt.show()
X.head()

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

def replace_outliers_with_median(df, columns):
    for column in columns:
        median = df[column].median()  # Compute median from the original DataFrame
        IQR = df[column].quantile(0.75) - df[column].quantile(0.25)
        lower_limit = df[column].quantile(0.25) - 1.5 * IQR
        upper_limit = df[column].quantile(0.75) + 1.5 * IQR
        df[column] = np.where(
            (df[column] < lower_limit) | (df[column] > upper_limit),
            median,  # Use the computed median
            df[column]
        )
    return df

# Assuming X_SS is your DataFrame and X_SS.columns are the columns you want to process
X = replace_outliers_with_median(X, X.columns)

# Plot boxplots to visualize the effect of outlier replacement
sns.boxplot(data=X)
plt.show()

# Display the head and tail of the DataFrame to see the changes
print(X.head())
print(X.tail())

from sklearn.linear_model import Lasso, Ridge
ridge = Ridge(alpha=2.0)
ridge.fit(X, Y)
print("Ridge coefficients:", ridge.coef_)


lasso = Lasso(alpha=0.01)
lasso.fit(X, Y)
print("Lasso coefficients:", lasso.coef_)

import pandas as pd
from statsmodels.stats.outliers_influence import variance_inflation_factor as vif

vif_data = pd.DataFrame()
vif_data["features"] = X.columns
vif_data["VIF Factor"] = [vif(X.values, i) for i in range(X.shape[1])]

print(vif_data)

from sklearn.model_selection  import train_test_split
x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.3)
from sklearn.tree import DecisionTreeRegressor
df = DecisionTreeRegressor(random_state=0)
df.fit(x_train,y_train)
y_pred_train = df.predict(x_train)
y_pred_test = df.predict(x_test)
from sklearn.metrics import mean_squared_error
import numpy as np
y_pred_train = mean_squared_error(y_train,y_pred_train)
y_pred_test = mean_squared_error(y_test,y_pred_test)
print('root mean squre error for Y pred train',np.sqrt(y_pred_train))
print('root mean squre error for Y pred test',np.sqrt(y_pred_test))

from sklearn.model_selection  import train_test_split
x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.3)
from sklearn.ensemble import RandomForestRegressor
df = RandomForestRegressor(random_state=0)
df.fit(x_train,y_train)
y_pred_train = df.predict(x_train)
y_pred_test = df.predict(x_test)
from sklearn.metrics import mean_squared_error
import numpy as np
y_pred_train = mean_squared_error(y_train,y_pred_train)
y_pred_test = mean_squared_error(y_test,y_pred_test)
print('root mean squre error for Y pred train',np.sqrt(y_pred_train))
print('root mean squre error for Y pred test',np.sqrt(y_pred_test))

for i in range(100):
  x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.3,random_state = i)
  df =  RandomForestRegressor()
  df.fit(x_train,y_train)
  y_pred_train = df.predict(x_train)
  y_pred_test = df.predict(x_test)
  y_pred_train = mean_squared_error(y_train,y_pred_train)
  y_pred_test = mean_squared_error(y_test,y_pred_test)
print('root mean squre error for Y pred train',np.sqrt(y_pred_train))
print('root mean squre error for Y pred test',np.sqrt(y_pred_test))

import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

best_max_samples = None
best_n_estimators = None
best_max_features = None
best_score = np.inf

for max_samples in range(10, 101, 10):
    for n_estimators in range(10, 101, 10):
        for max_features in range(1, len(X.columns) + 1):
            x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)
            model = RandomForestRegressor(max_samples=max_samples, n_estimators=n_estimators, max_features=max_features)
            model.fit(x_train, y_train)
            y_pred = model.predict(x_test)
            mse = mean_squared_error(y_test, y_pred)
            if mse < best_score:
                best_max_samples = max_samples
                best_n_estimators = n_estimators
                best_max_features = max_features
                best_score = mse

print("Best max_samples:", best_max_samples)
print("Best n_estimators:", best_n_estimators)
print("Best max_features:", best_max_features)
print("Best MSE:", best_score)

for i in range(100):
  x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.3,random_state = i)
  df =  RandomForestRegressor(max_samples=100, n_estimators=90, max_features=10)
  df.fit(x_train,y_train)
  y_pred_train = df.predict(x_train)
  y_pred_test = df.predict(x_test)
  y_pred_train = mean_squared_error(y_train,y_pred_train)
  y_pred_test = mean_squared_error(y_test,y_pred_test)
print('root mean squre error for Y pred train',np.sqrt(y_pred_train))
print('root mean squre error for Y pred test',np.sqrt(y_pred_test))