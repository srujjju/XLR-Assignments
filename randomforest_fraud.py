# -*- coding: utf-8 -*-
"""RandomForest_Fraud

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18hofOtnTLo7ThTOW-4LSx0-dq6mWSb9A
"""

from google.colab import files

uploaded = files.upload()

import pandas as pd
file = pd.read_csv('Fraud_check.csv')
file.head()

import numpy as np
file['tax_status'] = np.where(file['Taxable.Income'] <= 30000, 'Risky', 'Good')
file.head()

X = file.iloc[:,:-1]
Y = file.iloc[:,-1]
X.head()
print(X.head())

import seaborn as sns
import matplotlib.pyplot as plt

sns.pairplot(file)
plt.show()

# Calculate correlation matrix
correlation_matrix = file.corr()

# Visualize correlation matrix using heatmap
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm")
plt.show()

print(X.head())

from sklearn.preprocessing import StandardScaler,LabelEncoder
SS = StandardScaler()
LR = LabelEncoder()
selected_columns_ss = ['Taxable.Income','City.Population','Work.Experience']
selected_columns_lr = ['Undergrad','Marital.Status','Urban']
X[selected_columns_ss] = SS.fit_transform(X[selected_columns_ss])
for columns in selected_columns_lr:
  X[columns] = LR.fit_transform(X[columns])
Y = LR.fit_transform(Y)
X.head()

X.isnull().sum()

import numpy as np

sns.boxplot(data=X)
plt.show()
X.head()

from sklearn.linear_model import Lasso, Ridge
ridge = Ridge(alpha=2.0)
ridge.fit(X, Y)
print("Ridge coefficients:", ridge.coef_)


lasso = Lasso(alpha=0.01)
lasso.fit(X, Y)
print("Lasso coefficients:", lasso.coef_)

X = X.drop(['Undergrad','Marital.Status','Work.Experience'] ,axis=1)

X.head()

import pandas as pd
from statsmodels.stats.outliers_influence import variance_inflation_factor as vif

vif_data = pd.DataFrame()
vif_data["features"] = X.columns
vif_data["VIF Factor"] = [vif(X.values, i) for i in range(X.shape[1])]

print(vif_data)

from sklearn.model_selection  import train_test_split
x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.3)
from sklearn.tree import DecisionTreeClassifier
df = DecisionTreeClassifier()
df.fit(x_train,y_train)
y_pred_train = df.predict(x_train)
y_pred_test = df.predict(x_test)

from sklearn.metrics import mean_squared_error
import numpy as np
y_pred_train = mean_squared_error(y_train,y_pred_train)
y_pred_test = mean_squared_error(y_test,y_pred_test)
print('root mean squre error for Y pred train',np.sqrt(y_pred_train))
print('root mean squre error for Y pred test',np.sqrt(y_pred_test))

from sklearn.model_selection  import train_test_split
x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.3)
from sklearn.ensemble import RandomForestClassifier
df = RandomForestClassifier(random_state=0)
df.fit(x_train,y_train)
y_pred_train = df.predict(x_train)
y_pred_test = df.predict(x_test)
from sklearn.metrics import mean_squared_error
import numpy as np
y_pred_train = mean_squared_error(y_train,y_pred_train)
y_pred_test = mean_squared_error(y_test,y_pred_test)
print('root mean squre error for Y pred train',np.sqrt(y_pred_train))
print('root mean squre error for Y pred test',np.sqrt(y_pred_test))

for i in range(100):
  x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.3,random_state = i)
  df =  RandomForestClassifier()
  df.fit(x_train,y_train)
  y_pred_train = df.predict(x_train)
  y_pred_test = df.predict(x_test)
  y_pred_train = mean_squared_error(y_train,y_pred_train)
  y_pred_test = mean_squared_error(y_test,y_pred_test)
print('root mean squre error for Y pred train',np.sqrt(y_pred_train))
print('root mean squre error for Y pred test',np.sqrt(y_pred_test))

import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

best_max_samples = None
best_n_estimators = None
best_max_features = None
best_score = np.inf

for max_samples in range(10, 101, 10):
    for n_estimators in range(10, 101, 10):
        for max_features in range(1, len(X.columns) + 1):
            x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)
            model = RandomForestClassifier(max_samples=max_samples, n_estimators=n_estimators, max_features=max_features)
            model.fit(x_train, y_train)
            y_pred = model.predict(x_test)
            mse = mean_squared_error(y_test, y_pred)
            if mse < best_score:
                best_max_samples = max_samples
                best_n_estimators = n_estimators
                best_max_features = max_features
                best_score = mse

print("Best max_samples:", best_max_samples)
print("Best n_estimators:", best_n_estimators)
print("Best max_features:", best_max_features)
print("Best MSE:", best_score)

for i in range(100):
  x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.3,random_state = i)
  df =  RandomForestClassifier(max_samples=10, n_estimators=60, max_features=3)
  df.fit(x_train,y_train)
  y_pred_train = df.predict(x_train)
  y_pred_test = df.predict(x_test)
  y_pred_train = mean_squared_error(y_train,y_pred_train)
  y_pred_test = mean_squared_error(y_test,y_pred_test)
print('root mean squre error for Y pred train',np.sqrt(y_pred_train))
print('root mean squre error for Y pred test',np.sqrt(y_pred_test))